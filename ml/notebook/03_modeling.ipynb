{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02b62056",
   "metadata": {},
   "source": [
    "# Modeling for Predictive Maintenance\n",
    "\n",
    "In this step, we build machine learning models to predict machine failures using both raw and engineered features.  \n",
    "Since the dataset is imbalanced (only ~4% failures), model choice and evaluation focus on **recall for the failure class** â€” catching failures is more important than avoiding false alarms.  \n",
    "\n",
    "## Why start with baselines?\n",
    "\n",
    "- **Logistic Regression**  \n",
    "  - A linear, interpretable model.  \n",
    "  - Serves as a baseline to check whether engineered features expose the failure rules clearly enough for a simple classifier.  \n",
    "  - With class balancing, we expect it to perform surprisingly well.\n",
    "\n",
    "- **Random Forest**  \n",
    "  - A non-linear tree ensemble that captures feature interactions automatically.  \n",
    "  - More flexible than Logistic Regression, often stronger on tabular datasets.  \n",
    "  - Still explainable (feature importance, decision paths).\n",
    "\n",
    "By comparing these two baselines, we establish reference performance before moving to an advanced gradient boosting model (XGBoost).\n",
    "\n",
    "**Evaluation metrics**  \n",
    "- **Precision (failures)**: How many predicted failures are truly failures?  \n",
    "- **Recall (failures)**: How many actual failures did we detect? (most critical metric here)  \n",
    "- **F1-score**: Balance of precision and recall.  \n",
    "- **Confusion matrix**: Visual breakdown of predictions.\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ‘‰ After baselines, we will train **XGBoost** to see if a gradient boosting approach improves recall and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64803fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['udi', 'product_id', 'air_temperature_[k]', 'process_temperature_[k]',\n",
      "       'rotational_speed_[rpm]', 'torque_[nm]', 'tool_wear_[min]',\n",
      "       'machine_failure', 'twf', 'hdf', 'pwf', 'osf', 'rnf', 'temp_diff',\n",
      "       'power', 'wear_torque', 'norm_wear', 'type_L', 'type_M'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the engineered dataset\n",
    "df = pd.read_csv(\"/Users/swetha/predictive-maintenance-etl-ml/data/ai4i2020_featured.csv\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d94462e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (10000, 16)\n",
      "Target distribution:\n",
      " machine_failure\n",
      "0    9661\n",
      "1     339\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y = df[\"machine_failure\"]\n",
    "X = df.drop([\"machine_failure\", \"udi\", \"product_id\"], axis=1, errors=\"ignore\")\n",
    "print(\"Feature shape:\", X.shape)\n",
    "print(\"Target distribution:\\n\", y.value_counts()) #machine_failure = 0 (no failure) or 1 (failure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c939f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 8000\n",
      "Test samples: 2000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "print(\"Training samples:\", X_train.shape[0])\n",
    "print(\"Test samples:\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84656d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1932\n",
      "           1       1.00      0.97      0.99        68\n",
      "\n",
      "    accuracy                           1.00      2000\n",
      "   macro avg       1.00      0.99      0.99      2000\n",
      "weighted avg       1.00      1.00      1.00      2000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1932    0]\n",
      " [   2   66]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=5000, class_weight=\"balanced\", solver=\"saga\")\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = log_reg.predict(X_test_scaled)\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9afde911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1932\n",
      "           1       1.00      0.93      0.96        68\n",
      "\n",
      "    accuracy                           1.00      2000\n",
      "   macro avg       1.00      0.96      0.98      2000\n",
      "weighted avg       1.00      1.00      1.00      2000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1932    0]\n",
      " [   5   63]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,    \n",
    "    class_weight=\"balanced\", \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Results:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add65d14",
   "metadata": {},
   "source": [
    "# Baseline Modeling Insights\n",
    "\n",
    "### Logistic Regression (with scaling + class balancing)\n",
    "- Achieved **~97% recall** on failures (66/68 detected).  \n",
    "- Precision was also very high (no false alarms).  \n",
    "- Shows that the engineered features expose the underlying failure rules clearly enough for even a linear model.  \n",
    "\n",
    "### Random Forest\n",
    "- Achieved **~93% recall** on failures (63/68 detected).  \n",
    "- Slightly lower recall than Logistic Regression but still strong overall accuracy.  \n",
    "- Tree-based splits capture non-linear relationships but may sacrifice recall in favor of precision.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "- **Feature engineering matters**: By encoding domain-specific rules (e.g., power, wear Ã— torque), we allowed simple models to perform almost perfectly.  \n",
    "- **Logistic Regression > Random Forest** on this dataset: an unusual but valuable insight that highlights the effect of synthetic deterministic rules.  \n",
    "- **Recall is critical**: In predictive maintenance, missing a failure (false negative) is much more costly than a false alarm (false positive). Logistic Regression balanced this trade-off better in our case.  \n",
    "\n",
    "---\n",
    "\n",
    "ðŸ‘‰ Next, we will train an **advanced model (XGBoost)** to evaluate whether gradient boosting can further improve recall and robustness compared to these baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b276cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "df.columns = (df.columns\n",
    "                .str.strip()\n",
    "                .str.lower()\n",
    "                .str.replace(\" \", \"_\")\n",
    "                .str.replace(r\"[\\[\\]<>]\", \"\", regex=True))\n",
    "\n",
    "X = df.drop([\"machine_failure\",\"udi\",\"product_id\"], axis=1, errors=\"ignore\")\n",
    "y = df[\"machine_failure\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77a66210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
      "Best params: {'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}\n",
      "CV best F1: 0.9868436928250013\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1932\n",
      "           1       1.00      0.97      0.99        68\n",
      "\n",
      "    accuracy                           1.00      2000\n",
      "   macro avg       1.00      0.99      0.99      2000\n",
      "weighted avg       1.00      1.00      1.00      2000\n",
      "\n",
      "[[1932    0]\n",
      " [   2   66]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import xgboost as xgb\n",
    "\n",
    "scale_pos_weight = (len(y_train) - sum(y_train)) / sum(y_train)\n",
    "\n",
    "xgb_base = xgb.XGBClassifier(\n",
    "    random_state=42,\n",
    "    eval_metric=\"logloss\",\n",
    "    tree_method=\"hist\",\n",
    "    scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [200, 300, 500],\n",
    "    \"learning_rate\": [0.03, 0.05, 0.1],\n",
    "    \"max_depth\": [3, 4, 5, 6],\n",
    "    \"subsample\": [0.7, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scorer = make_scorer(f1_score, average=\"binary\")\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=xgb_base,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scorer,\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"CV best F1:\", grid.best_score_)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5548dfdd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb_clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m train_sizes, train_scores, test_scores = learning_curve(\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[43mxgb_clf\u001b[49m, X, y, cv=\u001b[32m5\u001b[39m, scoring=\u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m,\n\u001b[32m      7\u001b[39m     train_sizes=np.linspace(\u001b[32m0.1\u001b[39m, \u001b[32m1.0\u001b[39m, \u001b[32m5\u001b[39m)\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m plt.plot(train_sizes, np.mean(train_scores, axis=\u001b[32m1\u001b[39m), label=\u001b[33m\"\u001b[39m\u001b[33mTrain F1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m plt.plot(train_sizes, np.mean(test_scores, axis=\u001b[32m1\u001b[39m), label=\u001b[33m\"\u001b[39m\u001b[33mTest F1\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'xgb_clf' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    xgb_clf, X, y, cv=5, scoring=\"f1\", n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5)\n",
    ")\n",
    "\n",
    "plt.plot(train_sizes, np.mean(train_scores, axis=1), label=\"Train F1\")\n",
    "plt.plot(train_sizes, np.mean(test_scores, axis=1), label=\"Test F1\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve - XGBoost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbb39b8",
   "metadata": {},
   "source": [
    "XGBoost achieved a macro F1-score of 0.99, detecting 97% of machine failures with no false alarms. \n",
    "However, this performance was almost identical to Logistic Regression, indicating that the datasetâ€™s \n",
    "failure patterns were already well captured by the engineered features. \n",
    "\n",
    "This shows that in predictive maintenance tasks, **domain-informed feature engineering can sometimes \n",
    "level the playing field between simple linear models and advanced ensemble methods**. While XGBoost \n",
    "remains a strong and robust choice, the marginal gain here highlights that feature design is often \n",
    "as important as model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9995d659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model and feature list exported successfully.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import json\n",
    "\n",
    "joblib.dump(best_model, \"/Users/swetha/predictive-maintenance-etl-ml/ml/xgb_model.pkl\")\n",
    "\n",
    "# Save feature list (to ensure consistent column order during scoring)\n",
    "feature_list = list(X_train.columns)\n",
    "with open(\"/Users/swetha/predictive-maintenance-etl-ml/ml/feature_list.json\", \"w\") as f:\n",
    "    json.dump(feature_list, f)\n",
    "\n",
    "print(\" Model and feature list exported successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
